# -*- coding: utf-8 -*-
"""rag_pipeline

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16iB43VUjvQ2ld0HWupHqAMXrlbgfRFSE
"""

from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain.llms import HuggingFacePipeline
import torch

# Load LLM
def load_llm():
    model_id = "mistralai/Mistral-7B-Instruct-v0.2"
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", torch_dtype=torch.float16)
    pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, max_new_tokens=512)
    return HuggingFacePipeline(pipeline=pipe)

# Load vectorstore
def load_vectorstore():
    embedding_model = HuggingFaceEmbeddings(model_name="pritamdeka/S-PubMedBERT-MS-MARCO")
    return FAISS.load_local("project/faiss_index", embedding_model, allow_dangerous_deserialization=True)

# Custom Prompt
custom_prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="""You are a helpful biomedical assistant. Answer based only on the given context. If the answer isn't there, say "I don't know".

{context}
Question: {question}
Answer:"""
)

# Load RAG pipeline
def load_rag_chain():
    llm = load_llm()
    db = load_vectorstore()
    return RetrievalQA.from_chain_type(
        llm=llm,
        retriever=db.as_retriever(search_kwargs={"k": 4}),
        chain_type_kwargs={"prompt": custom_prompt},
        return_source_documents=False
    )

# Query Function
def query_rag(rag_chain, question):
    response = rag_chain.invoke({"query": question})
    return response["result"].strip()